{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import\n",
    "\n",
    "import pandas as pd\n",
    "import urllib,json\n",
    "from pandas.io.json import json_normalize\n",
    "import os\n",
    "import inquirer\n",
    "from datetime import datetime\n",
    "from sklearn import preprocessing\n",
    "from PySAM.PySSC import PySSC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "from time import strptime\n",
    "import calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def API_Reader(url):\n",
    "    request = urllib.request.Request(url)\n",
    "    response = urllib.request.urlopen(request)\n",
    "    json_load = json.loads(response.read())\n",
    "    return(json_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eia_api_key = 'b0cbfe0b334c7c98b857879d54226b4a'\n",
    "ei_api_key = 'o8dCYVBSkm1c2tnB5YUAYsKv4iZaC8AkyPYMYSBL'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rate Pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pay_Structure_DF = pd.DataFrame()\n",
    "y = range(0,100)\n",
    "for x in y:\n",
    "    print(x),\n",
    "    if x == 0:\n",
    "        offset=0\n",
    "    if x ==1:\n",
    "        offset=501\n",
    "    if x>1:\n",
    "        offset=501 +((x-1)*500)\n",
    "    api_type = 'csv'\n",
    "    EI_url = 'https://api.openei.org/utility_rates?version=latest&format='+api_type+'&limit=500&sector=Industrial&detail=full&offset='+str(offset)+'&api_key=' + ei_api_key\n",
    "    EI_API_CSV = pd.read_csv(EI_url)\n",
    "    Pay_Structure_DF = pd.concat([Pay_Structure_DF,EI_API_CSV], axis=0, join='outer', ignore_index=True, keys=None,levels=None, names=None, verify_integrity=False, copy=True)\n",
    "    if EI_API_CSV.shape[0] < 500:\n",
    "            break\n",
    "Pay_Structure_DF.to_csv('Pay_Structure.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_columns = ['label','uri',\n",
    "    'approved',\n",
    "    'is_default',\n",
    "    'name',\n",
    "    'startdate',\n",
    "    'enddate',\n",
    "    'utility',\n",
    "    'eiaid',\n",
    "    'country',\n",
    "    'sector',\n",
    "    'servicetype',\n",
    "    'description',\n",
    "    'source',\n",
    "    'sourceparent',\n",
    "    'revisions']\n",
    "coincident_columns = list(Pay_Structure_DF.columns[Pay_Structure_DF.columns.str.contains('coincident')])# + ['coincidentratestructure/period2/tier0rate']\n",
    "needed_columns = base_columns + coincident_columns\n",
    "Coincident_DF = Pay_Structure_DF.loc[Pay_Structure_DF['coincidentrateunit'][Pay_Structure_DF['coincidentrateunit'].notna()].index,needed_columns ]\n",
    "Coincident_DF.reset_index(inplace = True,drop = True)\n",
    "Coincident_DF.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coincidentschedulecolumns = Coincident_DF.columns[Coincident_DF.columns.str.contains('coincidentschedule')]\n",
    "id_columns = [x for x in needed_columns if x not in coincidentschedulecolumns]\n",
    "Coincident_DF_Schedule = pd.melt(Coincident_DF,id_vars = id_columns,value_vars = coincidentschedulecolumns,var_name = 'month_hour',value_name = 'schedule')\n",
    "Coincident_DF_Schedule.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Coincident_DF_Schedule['month'] = Coincident_DF_Schedule.month_hour.str.split('_').str[1]\n",
    "Coincident_DF_Schedule['hour'] = Coincident_DF_Schedule.month_hour.str.split('_').str[2]\n",
    "d= {'jan':1, 'feb':2, 'mar':3, 'apr':4, 'may':5, 'jun':6, 'jul':7, 'aug':8, 'sep':9,\n",
    "       'oct':10, 'nov':11, 'dec':12}\n",
    "Coincident_DF_Schedule['month'].replace(d,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Coincident_DF_Schedule.loc[Coincident_DF_Schedule['schedule'] == 0,'price'] = Coincident_DF_Schedule['coincidentratestructure/period0/tier0rate']\n",
    "Coincident_DF_Schedule.loc[Coincident_DF_Schedule['schedule'] == 1,'price'] = Coincident_DF_Schedule['coincidentratestructure/period1/tier0rate']\n",
    "Coincident_DF_Schedule.loc[Coincident_DF_Schedule['schedule'] == 2,'price'] = Coincident_DF_Schedule['coincidentratestructure/period2/tier0rate']\n",
    "Coincident_DF_Schedule.hour = Coincident_DF_Schedule.hour.astype(int)\n",
    "Coincident_DF_Schedule.month = Coincident_DF_Schedule.month.astype(int)\n",
    "Coincident_DF_Schedule.sort_values(by = ['utility','month','hour'],inplace = True)\n",
    "Coincident_DF_Schedule.reset_index(inplace = True,drop = True)\n",
    "CP_Utilities = Coincident_DF.eiaid.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Coincident_DF_Schedule.utility.unique())\n",
    "Coincident_DF_Schedule['utility'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Utitility_Rules = pd.read_csv('https://raw.githubusercontent.com/awendell989/EIA_2018_Utiltiy_Policies/master/Dynamic_Pricing_2018_Avery.csv')\n",
    "Utitility_Rules.loc[:,Utitility_Rules.columns[Utitility_Rules.columns.str.contains('--')]] = Utitility_Rules.loc[:,Utitility_Rules.columns[Utitility_Rules.columns.str.contains('--')]].replace({'N':False,'Y':True})\n",
    "Industrial_Columns = Utitility_Rules.select_dtypes(include='bool').columns\n",
    "Industrial_Columns = Industrial_Columns[Industrial_Columns.str.contains('Industrial')]\n",
    "Utitility_Rules['Industrial_Total'] = Utitility_Rules[Industrial_Columns].sum(axis = 1)\n",
    "CP_Options = Industrial_Columns + ['Industrial_Total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Utitility_Rules.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Coincident_DF = Coincident_DF.merge(Utitility_Rules[['Utility Number','BA Code']],left_on = 'eiaid',right_on = 'Utility Number')\n",
    "Coincident_DF_Schedule_Full = Coincident_DF_Schedule.merge(Utitility_Rules[['Utility Number','BA Code']],left_on = 'eiaid',right_on = 'Utility Number')\n",
    "CP_BAs = Coincident_DF_Schedule_Full['BA Code'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Coincident_DF_Schedule_Full.eiaid.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demand Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demand_total_url = 'http://api.eia.gov/category/?api_key=b0cbfe0b334c7c98b857879d54226b4a&category_id=2122628'\n",
    "json_load = API_Reader(demand_total_url)\n",
    "Categories = pd.DataFrame(json_load['category']['childcategories'])\n",
    "Categories['BA_Code'] = Categories['name'].apply(lambda r: r.split('(')[1].split(')')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Categories = Categories.loc[Categories['BA_Code'].isin(CP_BAs),:].reset_index(drop = True)\n",
    "Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Demand_Data_Total = pd.DataFrame(columns = ['UTC', 'MWh', 'BA', 'CID'])\n",
    "for x in range(0,max(Categories.index+1)):\n",
    "    name = Categories.iloc[x,1]\n",
    "    category_id = Categories.iloc[x,0]\n",
    "    category_url ='http://api.eia.gov/category/?api_key='+ str(eia_api_key) +'&category_id='+str(category_id)\n",
    "    BA_Data = API_Reader(category_url)\n",
    "    Series = pd.DataFrame(BA_Data['category']['childseries'])\n",
    "    Series_id = Series.loc[Series['name'].str.contains('UTC'),'series_id'][0]\n",
    "    series_url = 'https://api.eia.gov/series/?api_key='+eia_api_key+'&series_id='+str(Series_id)\n",
    "    Series_Data = API_Reader(series_url)\n",
    "    step1 = Series_Data['series'][0]\n",
    "    step2 = step1['data']\n",
    "    Columns = ['UTC','MWh']\n",
    "    DF = pd.DataFrame(step2,columns = Columns)\n",
    "    DF['BA'] = name\n",
    "    DF['CID'] = category_id\n",
    "    Demand_Data_Total = pd.concat([Demand_Data_Total,DF], axis=0, join='outer', ignore_index=True, keys=None,levels=None, names=None, verify_integrity=False, copy=True)\n",
    "    print(name + ' added!')\n",
    "    Demand_Data_Total['UTC'] = pd.to_datetime(Demand_Data_Total['UTC'],utc=True) #Is this True DOh?\n",
    "    Demand_Data_Total['BA Code'] = Demand_Data_Total['BA'].apply(lambda r: r.split('(')[1].split(')')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Demand_Data_Total['hour'] = Demand_Data_Total['UTC'].dt.hour\n",
    "Demand_Data_Total['hour_p'] = Demand_Data_Total['UTC'].dt.hour/24\n",
    "Demand_Data_Total['month'] = Demand_Data_Total['UTC'].dt.month\n",
    "Demand_Data_Total['year'] = Demand_Data_Total['UTC'].dt.year\n",
    "Demand_Data_Total['day'] = Demand_Data_Total['UTC'].dt.day\n",
    "Demand_Data_Total['Rank'] = Demand_Data_Total.groupby(['BA','month','year'],axis = 0)['MWh'].rank(method = 'max',ascending = False)\n",
    "Demand_Data_Total['Day_Hour'] = Demand_Data_Total['day'] + Demand_Data_Total['hour_p']\n",
    "current_month = datetime.now().month\n",
    "current_year = datetime.now().year\n",
    "Demand_Data_Total = Demand_Data_Total[~((Demand_Data_Total['month'] == current_month) & (Demand_Data_Total['year'] == current_year))]\n",
    "Demand_Data_Total = Demand_Data_Total.loc[Demand_Data_Total['BA Code'].isin(CP_BAs),:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Month_Min_Max = Demand_Data_Total.groupby(['BA','year','month'],axis = 0).agg({'MWh' : ['min','max']}).reset_index()\n",
    "Demand_Data_Total = Demand_Data_Total.merge(Month_Min_Max,on =['BA','year','month'])\n",
    "Demand_Data_Total.rename(columns = {Demand_Data_Total.columns[-1] : 'm_max',Demand_Data_Total.columns[-2] : 'm_min'},inplace = True)\n",
    "Demand_Data_Total['MWH_M_Scale'] = Demand_Data_Total.apply(lambda x: (x['MWh'] - x['m_min']) / (x['m_max'] - x['m_min']),axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Demand_Data_Total = Demand_Data_Total.loc[~((Demand_Data_Total['month'] == 2) & (Demand_Data_Total['day'] == 29)) ,]\n",
    "Time_DF = Demand_Data_Total.groupby(['month','day','hour']).size().reset_index().drop(0,axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Demand_Data_Total.to_csv('Demand_by_BA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Demand_Data_Total_Agg = Demand_Data_Total.groupby(['BA', 'BA Code', 'CID','month','day','hour','Day_Hour'])['MWH_M_Scale','MWh'].sum()\n",
    "Demand_Data_Total_Agg = Demand_Data_Total.groupby(['BA', 'BA Code', 'CID','month','hour'])['MWH_M_Scale','MWh'].mean()\n",
    "Demand_Data_Total_Agg.reset_index(inplace = True)\n",
    "Demand_Data_Total_Agg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge to Rate Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Demand_Rate_Schedule = Demand_Data_Total_Agg.merge(Coincident_DF_Schedule_Full, how='right', left_on=['month','hour','BA Code'], right_on=['month','hour','BA Code'])\n",
    "Demand_Rate_Schedule = Demand_Rate_Schedule[['BA Code','utility','eiaid','name','month','hour','MWh','MWH_M_Scale','schedule','price']].sort_values(by =['BA Code','utility','eiaid','name','month','hour'] ).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Demand_Rate_Schedule.to_csv('Demand_Rate_Schedule.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Demand_Rate_Schedule.eiaid.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zipcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_to_Z_I = pd.read_csv('iouzipcodes2011.csv',dtype = {'zip': str})\n",
    "U_to_Z_NI = pd.read_csv('noniouzipcodes2011.csv',dtype = {'zip': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concat And Run\n",
    "\n",
    "U_to_Z = pd.concat([U_to_Z_I,U_to_Z_NI])\n",
    "U_to_Z = U_to_Z.drop_duplicates(subset=['zip','eiaid'],keep = 'last').sort_values(by = 'zip')\n",
    "#U_to_Z_Zips = U_to_Z.loc[U_to_Z['eiaid'].isin(CP_Utilities),'zip']\n",
    "file = os.path.join(os.getcwd(),'CP_Zips.csv')\n",
    "U_to_Z.to_csv(file,index = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_to_Z.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Demand_Rate_Zip_Schedule = Demand_Rate_Schedule.merge(U_to_Z[['zip','eiaid']],on ='eiaid',how ='left')\n",
    "Demand_Rate_Zip_Schedule.sort_values(by =['BA Code','utility','eiaid','name','zip','month','hour'],inplace = True)\n",
    "Demand_Rate_Zip_Schedule.dropna(inplace = True)\n",
    "Demand_Rate_Zip_Schedule.reset_index(inplace = True,drop = True)\n",
    "Demand_Rate_Zip_Schedule.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Demand_Rate_Zip_Schedule.eiaid.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CP_Zips = Demand_Rate_Zip_Schedule.zip.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solar Power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Weather_Files = pd.DataFrame(os.listdir(os.path.join(os.getcwd(),'Weather_Files')),columns = ['Loc'])\n",
    "Weather_Files = pd.concat([Weather_Files,Weather_Files.Loc.str.split('_',expand = True).rename(columns = {0:'Zip',1:'Lat',2:'Long'})],axis = 1)\n",
    "Weather_Files_Zips = Weather_Files.Zip\n",
    "Zip_Daily_Hourly_Solar_Production = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Need_WF_Zips = list(set(CP_Zips) - set(Weather_Files_Zips))\n",
    "for x in Need_WF_Zips:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(CP_Zips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Weather_Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in CP_Zips:\n",
    "    print(m)\n",
    "    Loc = Weather_Files.loc[Weather_Files['Zip'] == m,'Loc'].values[0]\n",
    "    file = os.path.join(os.getcwd(),'Weather_Files',Loc)\n",
    "    ssc = PySSC()\n",
    "    print ('Current folder = /Users/Avery/Desktop/Coincidenet_Peak_Project/PySimple2')\n",
    "    print ('SSC Version = ', ssc.version())\n",
    "    ssc.module_exec_set_print(0)\n",
    "    data = ssc.data_create()\n",
    "    ssc.data_set_string( data, b'solar_resource_file',str.encode(file));  #'b/Users/Avery/Desktop/Coincident_Peak_Project/Weather_Files/17302_39.810052_-76.408374_psmv3_60_tmy.csv' );\n",
    "    ssc.data_set_number( data, b'system_capacity', 10000 )\n",
    "    ssc.data_set_number( data, b'module_type', 0 )\n",
    "    ssc.data_set_number( data, b'dc_ac_ratio', 1.1499999761581421 )\n",
    "    ssc.data_set_number( data, b'inv_eff', 96 )\n",
    "    ssc.data_set_number( data, b'losses', 14.075660705566406 )\n",
    "    ssc.data_set_number( data, b'array_type', 0 )\n",
    "    ssc.data_set_number( data, b'tilt', 20 )\n",
    "    ssc.data_set_number( data, b'azimuth', 180 )\n",
    "    ssc.data_set_number( data, b'gcr', 0.40000000596046448 )\n",
    "    ssc.data_set_number( data, b'adjust:constant', 0)\n",
    "    module = ssc.module_create(b'pvwattsv5')\n",
    "    ssc.module_exec_set_print( 0 );\n",
    "    if ssc.module_exec(module, data) == 0:\n",
    "        print ('pvwattsv5 simulation error')\n",
    "        idx = 1\n",
    "        msg = ssc.module_log(module, 0)\n",
    "        while (msg != None):\n",
    "            print ('\t: ' + msg.decode(\"utf - 8\"))\n",
    "            msg = ssc.module_log(module, idx)\n",
    "            idx = idx + 1\n",
    "        SystemExit( \"Simulation Error\" );\n",
    "    ssc.module_free(module)\n",
    "    annual_energy = ssc.data_get_number(data, b'annual_energy');\n",
    "    print ('Annual energy (year 1) = ', annual_energy)\n",
    "    capacity_factor = ssc.data_get_number(data, b'capacity_factor');\n",
    "    print ('Capacity factor (year 1) = ', capacity_factor)\n",
    "    kwh_per_kw = ssc.data_get_number(data, b'kwh_per_kw');\n",
    "    print ('Energy yield (year 1) = ', kwh_per_kw)\n",
    "    hourly_data = pd.Series(ssc.data_get_array(data,b'gen'))\n",
    "    hourly_data = pd.concat([Time_DF,hourly_data],axis =  1)\n",
    "    hourly_data['zip'] = m\n",
    "    Zip_Daily_Hourly_Solar_Production = pd.concat([Zip_Daily_Hourly_Solar_Production,hourly_data],axis =0)\n",
    "Zip_Daily_Hourly_Solar_Production.rename(columns = {0:'Production'},inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Zip_Daily_Hourly_Solar_Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Demand Data =\n",
    "\n",
    "BA, Month, Hour (average)\n",
    "\n",
    "PV:\n",
    "\n",
    "Zip, Month, Day, Hour \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Zip_Hourly_Solar_Production = Zip_Daily_Hourly_Solar_Production.groupby(['zip','month','hour'])['Production'].mean().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scheduel_Demand_Production_DF = Demand_Rate_Zip_Schedule.merge(Zip_Hourly_Solar_Production,on = ['zip','month','hour'],how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scheduel_Demand_Production_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scheduel_Demand_Production_DF.to_csv('Scheduel_demand_Production.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
